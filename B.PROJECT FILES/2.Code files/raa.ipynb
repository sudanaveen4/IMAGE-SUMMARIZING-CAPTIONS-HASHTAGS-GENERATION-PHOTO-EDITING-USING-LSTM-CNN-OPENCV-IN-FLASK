{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703bf7a3e95540cdbcf7670b030dd907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq man in blue shirt and jeans is running on grassy field endseq\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm \n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input \n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add\n",
    "# Load vgg16 Model\n",
    "model = VGG16()\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "image = load_img(\"pexels-photo-1682649.jpeg\", target_size=(224, 224))\n",
    "    # convert image pixels to numpy array\n",
    "image = img_to_array(image)\n",
    "    # reshape data for model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocess image for vgg\n",
    "image = preprocess_input(image)\n",
    "fe=model.predict(image, verbose=0)\n",
    "\n",
    "# load features from pickle\n",
    "with open(os.path.join('features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)\n",
    "with open(os.path.join('input\\captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()\n",
    "\n",
    "# create mapping of image to captions\n",
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # convert caption list to string\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # delete digits, special chars, etc., \n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption\n",
    "\n",
    "clean(mapping)\n",
    "\n",
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "\n",
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]\n",
    "\n",
    "# create data generator to get data in batch (avoids session crash)\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # loop over images\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n += 1\n",
    "            captions = mapping[key]\n",
    "            # process each caption\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                    # store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                yield [X1, X2], y\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n = 0\n",
    "\n",
    "# encoder model\n",
    "# image feature layers\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "model = tf.keras.models.load_model('best_model.h5')\n",
    "\n",
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = predict_caption(model, fe, tokenizer, max_length)\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. I'm running through the grassy field in my blue shirt and jeans â€“ feeling free and alive! \n",
      "2. I'm sprinting through the lush green grass, the wind in my hair and the sun on my face. \n",
      "3. Nothing can stop me as I run through the grassy field in my blue shirt and jeans. \n",
      "4. I'm running with the wind, feeling the energy of the grassy field beneath my feet. \n",
      "5. I'm running through the grassy field, my blue shirt and jeans flowing with the breeze. \n",
      "6. I'm feeling alive and free as I run through the grassy field in my blue shirt and jeans. \n",
      "7. I'm running through the grassy field, my blue shirt and jeans waving in the wind. \n",
      "8. I'm running through the grassy field, feeling the wind on my face and the sun on my skin. \n",
      "9. I'm running through the grassy field, feeling the joy of the moment in my blue shirt and jeans. \n",
      "10. I'm running through the grassy field, feeling the freedom and the beauty of the moment. \n",
      "11. I'm running through the grassy field, feeling the energy of the sun and the wind in my blue shirt and jeans. \n",
      "12. I'm running through the grassy field, feeling the power of the moment in my blue shirt and jeans. \n",
      "13. I'm running through the grassy field, feeling the joy of the moment in my blue shirt and jeans. \n",
      "14. I'm running through the grassy field, feeling the power of the sun and the wind in my blue shirt and jeans. \n",
      "15. I'm running through the grassy field, feeling the freedom and the beauty of the moment in my blue shirt and jeans. \n",
      "16. I'm running through the grassy field, feeling the energy of the sun and the wind in my blue shirt and jeans. \n",
      "17. I'm running through the grassy field, feeling the joy of the moment in my blue shirt and jeans. \n",
      "18. I'm running through the grassy field, feeling the freedom and the beauty of the moment in my blue shirt and jeans. \n",
      "19. I'm running through the grassy field, my blue shirt and jeans flowing with the breeze, feeling the power of the moment. \n",
      "20. I'm running through the grassy field, feeling the freedom and joy of the moment in my blue shirt and jeans.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Define OpenAI API key \n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Set up the model and prompt\n",
    "model_engine = \"text-davinci-003\"\n",
    "prompt = \"generate 20 trending captions about\"+y_pred+\"in first persion\"\n",
    "\n",
    "# Generate a response\n",
    "completion = openai.Completion.create(engine=model_engine,prompt=prompt,max_tokens=1024,n=1,stop=None,temperature=0.5,)\n",
    "\n",
    "response = completion.choices[0].text\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
